PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master> python hs_model_v1.py
2021-09-23 10:04:14.706922: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cudart64_101.dll
(None, 512, 512, 1)
(None, 512, 512, 1)
2021-09-23 10:04:17.024295: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library nvcuda.dll
2021-09-23 10:04:17.057611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with p
roperties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-23 10:04:17.071237: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cudart64_101.dll
2021-09-23 10:04:17.086570: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cublas64_10.dll
2021-09-23 10:04:17.098023: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cufft64_10.dll
2021-09-23 10:04:17.106151: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library curand64_10.dll
2021-09-23 10:04:17.121504: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cusolver64_10.dll
2021-09-23 10:04:17.130724: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cusparse64_10.dll
2021-09-23 10:04:17.151038: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cudnn64_7.dll
2021-09-23 10:04:17.170440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu de
vices: 0
2021-09-23 10:04:17.175851: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is
 optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in perfor
mance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-23 10:04:17.204391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b38e554750 in
itialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-23 10:04:17.213334: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0):
 Host, Default Version
2021-09-23 10:04:17.220564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with p
roperties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-23 10:04:17.236096: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cudart64_101.dll
2021-09-23 10:04:17.243597: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cublas64_10.dll
2021-09-23 10:04:17.269628: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cufft64_10.dll
2021-09-23 10:04:17.279271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library curand64_10.dll
2021-09-23 10:04:17.288453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cusolver64_10.dll
2021-09-23 10:04:17.296346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cusparse64_10.dll
2021-09-23 10:04:17.304703: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cudnn64_7.dll
2021-09-23 10:04:17.313010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu de
vices: 0
2021-09-23 10:04:17.887546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect S
treamExecutor with strength 1 edge matrix:
2021-09-23 10:04:17.896143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2021-09-23 10:04:17.902205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2021-09-23 10:04:17.906145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow de
vice (/job:localhost/replica:0/task:0/device:GPU:0 with 8678 MB memory) -> physical GPU (device: 0, name:
GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-09-23 10:04:17.922615: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b3c77e6b00 in
itialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-09-23 10:04:17.933054: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0):
 GeForce GTX 1080 Ti, Compute Capability 6.1
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 512, 512, 1) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 64) 640         input_1[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 512, 512, 64) 36928       conv2d[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 64) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 256, 256, 128 147584      conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 128, 128, 256 590080      conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 64, 512)  2359808     conv2d_6[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 64, 64, 512)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 512)  0           dropout[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 1024) 4719616     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 1024) 9438208     conv2d_8[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 1024) 0           conv2d_9[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 64, 64, 1024) 0           dropout_1[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 64, 64, 512)  2097664     up_sampling2d[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 64, 64, 1024) 0           dropout[0][0]
                                                                 conv2d_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_11[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_1[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 128, 512 0           conv2d_5[0][0]
                                                                 conv2d_13[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 256 590080      conv2d_14[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_15[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_2[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256, 256, 256 0           conv2d_3[0][0]
                                                                 conv2d_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 128 295040      concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 128 147584      conv2d_17[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_18[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_3[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 512, 512, 128 0           conv2d_1[0][0]
                                                                 conv2d_19[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 512, 512, 2)  1154        conv2d_21[0][0]
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 512, 512, 1) 0
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 512, 512, 3)  0           conv2d_22[0][0]
                                                                 input_2[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 512, 512, 2)  56          concatenate_4[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 512, 512, 1)  19          conv2d_23[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 512, 512, 1)  2           conv2d_24[0][0]
==================================================================================================
Total params: 31,031,759
Trainable params: 31,031,759
Non-trainable params: 0
__________________________________________________________________________________________________
loading data
load train images...
------------------------------
load test images...
------------------------------
loading data done
(None, 512, 512, 1)
(None, 512, 512, 1)
got unet
Fitting model...
Epoch 1/50
2021-09-23 10:04:21.299823: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cudnn64_7.dll
2021-09-23 10:04:22.057605: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking
GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-09-23 10:04:22.117464: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully o
pened dynamic library cublas64_10.dll
  2/122 [..............................] - ETA: 31s - loss: 0.6936 - accuracy: 0.6990WARNING:tensorflow:Ca
llbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1685s vs `on_train_b
atch_end` time: 0.3560s). Check your callbacks.
122/122 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.7240
Epoch 00001: loss improved from inf to 0.68572, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 70s 577ms/step - loss: 0.6857 - accuracy: 0.7240 - val_loss: 0.
6824 - val_accuracy: 0.7277
Epoch 2/50
122/122 [==============================] - ETA: 0s - loss: 0.6693 - accuracy: 0.7491
Epoch 00002: loss improved from 0.68572 to 0.66935, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 561ms/step - loss: 0.6693 - accuracy: 0.7491 - val_loss: 0.
6814 - val_accuracy: 0.7234
Epoch 3/50
122/122 [==============================] - ETA: 0s - loss: 0.6720 - accuracy: 0.7420
Epoch 00003: loss did not improve from 0.66935
122/122 [==============================] - 68s 555ms/step - loss: 0.6720 - accuracy: 0.7420 - val_loss: 0.
6757 - val_accuracy: 0.7287
Epoch 4/50
122/122 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.7627
Epoch 00004: loss improved from 0.66935 to 0.65427, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.6543 - accuracy: 0.7627 - val_loss: 0.
6756 - val_accuracy: 0.7334
Epoch 5/50
122/122 [==============================] - ETA: 0s - loss: 0.6466 - accuracy: 0.7715
Epoch 00005: loss improved from 0.65427 to 0.64661, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 547ms/step - loss: 0.6466 - accuracy: 0.7715 - val_loss: 0.
6680 - val_accuracy: 0.7361
Epoch 6/50
122/122 [==============================] - ETA: 0s - loss: 0.6417 - accuracy: 0.7744
Epoch 00006: loss improved from 0.64661 to 0.64168, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.6417 - accuracy: 0.7744 - val_loss: 0.
6643 - val_accuracy: 0.7359
Epoch 7/50
122/122 [==============================] - ETA: 0s - loss: 0.6341 - accuracy: 0.7795
Epoch 00007: loss improved from 0.64168 to 0.63413, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 561ms/step - loss: 0.6341 - accuracy: 0.7795 - val_loss: 0.
6597 - val_accuracy: 0.7422
Epoch 8/50
122/122 [==============================] - ETA: 0s - loss: 0.6254 - accuracy: 0.7884
Epoch 00008: loss improved from 0.63413 to 0.62536, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.6254 - accuracy: 0.7884 - val_loss: 0.
6521 - val_accuracy: 0.7469
Epoch 9/50
122/122 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.7943
Epoch 00009: loss improved from 0.62536 to 0.61945, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 553ms/step - loss: 0.6194 - accuracy: 0.7943 - val_loss: 0.
6454 - val_accuracy: 0.7526
Epoch 10/50
122/122 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.7973
Epoch 00010: loss improved from 0.61945 to 0.61501, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 555ms/step - loss: 0.6150 - accuracy: 0.7973 - val_loss: 0.
6397 - val_accuracy: 0.7601
Epoch 11/50
122/122 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.8401
Epoch 00011: loss improved from 0.61501 to 0.58376, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 551ms/step - loss: 0.5838 - accuracy: 0.8401 - val_loss: 0.
5604 - val_accuracy: 0.8632
Epoch 12/50
122/122 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.8374
Epoch 00012: loss did not improve from 0.58376
122/122 [==============================] - 66s 543ms/step - loss: 0.5880 - accuracy: 0.8374 - val_loss: 0.
5990 - val_accuracy: 0.8207
Epoch 13/50
122/122 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.8588
Epoch 00013: loss improved from 0.58376 to 0.56361, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 555ms/step - loss: 0.5636 - accuracy: 0.8588 - val_loss: 0.
5392 - val_accuracy: 0.8794
Epoch 14/50
122/122 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.8733
Epoch 00014: loss improved from 0.56361 to 0.54929, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 554ms/step - loss: 0.5493 - accuracy: 0.8733 - val_loss: 0.
5457 - val_accuracy: 0.8434
Epoch 15/50
122/122 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.8752
Epoch 00015: loss improved from 0.54929 to 0.54258, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 551ms/step - loss: 0.5426 - accuracy: 0.8752 - val_loss: 0.
5276 - val_accuracy: 0.8845
Epoch 16/50
122/122 [==============================] - ETA: 0s - loss: 0.5317 - accuracy: 0.8867
Epoch 00016: loss improved from 0.54258 to 0.53165, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 553ms/step - loss: 0.5317 - accuracy: 0.8867 - val_loss: 0.
5262 - val_accuracy: 0.8894
Epoch 17/50
122/122 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.8937
Epoch 00017: loss improved from 0.53165 to 0.52168, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 553ms/step - loss: 0.5217 - accuracy: 0.8937 - val_loss: 0.
5152 - val_accuracy: 0.8908
Epoch 18/50
122/122 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.8976
Epoch 00018: loss improved from 0.52168 to 0.51689, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 551ms/step - loss: 0.5169 - accuracy: 0.8976 - val_loss: 0.
5302 - val_accuracy: 0.8782
Epoch 19/50
122/122 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.9029
Epoch 00019: loss improved from 0.51689 to 0.50798, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 557ms/step - loss: 0.5080 - accuracy: 0.9029 - val_loss: 0.
5014 - val_accuracy: 0.9039
Epoch 20/50
122/122 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.9086
Epoch 00020: loss improved from 0.50798 to 0.50011, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 560ms/step - loss: 0.5001 - accuracy: 0.9086 - val_loss: 0.
4974 - val_accuracy: 0.9079
Epoch 21/50
122/122 [==============================] - ETA: 0s - loss: 0.4934 - accuracy: 0.9122
Epoch 00021: loss improved from 0.50011 to 0.49339, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 560ms/step - loss: 0.4934 - accuracy: 0.9122 - val_loss: 0.
4941 - val_accuracy: 0.9066
Epoch 22/50
122/122 [==============================] - ETA: 0s - loss: 0.4897 - accuracy: 0.9124
Epoch 00022: loss improved from 0.49339 to 0.48972, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 560ms/step - loss: 0.4897 - accuracy: 0.9124 - val_loss: 0.
4876 - val_accuracy: 0.9120
Epoch 23/50
122/122 [==============================] - ETA: 0s - loss: 0.4845 - accuracy: 0.9145
Epoch 00023: loss improved from 0.48972 to 0.48447, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 69s 562ms/step - loss: 0.4845 - accuracy: 0.9145 - val_loss: 0.
4897 - val_accuracy: 0.9087
Epoch 24/50
122/122 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.9157
Epoch 00024: loss improved from 0.48447 to 0.47966, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 557ms/step - loss: 0.4797 - accuracy: 0.9157 - val_loss: 0.
4755 - val_accuracy: 0.9158
Epoch 25/50
122/122 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.9171
Epoch 00025: loss improved from 0.47966 to 0.47552, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4755 - accuracy: 0.9171 - val_loss: 0.
4736 - val_accuracy: 0.9153
Epoch 26/50
122/122 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.9188
Epoch 00026: loss improved from 0.47552 to 0.46973, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4697 - accuracy: 0.9188 - val_loss: 0.
4748 - val_accuracy: 0.9109
Epoch 27/50
122/122 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.9201
Epoch 00027: loss improved from 0.46973 to 0.46593, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4659 - accuracy: 0.9201 - val_loss: 0.
4689 - val_accuracy: 0.9141
Epoch 28/50
122/122 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.9211
Epoch 00028: loss improved from 0.46593 to 0.46154, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4615 - accuracy: 0.9211 - val_loss: 0.
4618 - val_accuracy: 0.9180
Epoch 29/50
122/122 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.9212
Epoch 00029: loss improved from 0.46154 to 0.45797, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4580 - accuracy: 0.9212 - val_loss: 0.
4606 - val_accuracy: 0.9160
Epoch 30/50
122/122 [==============================] - ETA: 0s - loss: 0.4549 - accuracy: 0.9218
Epoch 00030: loss improved from 0.45797 to 0.45489, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4549 - accuracy: 0.9218 - val_loss: 0.
4566 - val_accuracy: 0.9159
Epoch 31/50
122/122 [==============================] - ETA: 0s - loss: 0.4505 - accuracy: 0.9225
Epoch 00031: loss improved from 0.45489 to 0.45050, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4505 - accuracy: 0.9225 - val_loss: 0.
4555 - val_accuracy: 0.9166
Epoch 32/50
122/122 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.9226
Epoch 00032: loss improved from 0.45050 to 0.44793, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4479 - accuracy: 0.9226 - val_loss: 0.
4533 - val_accuracy: 0.9151
Epoch 33/50
122/122 [==============================] - ETA: 0s - loss: 0.4433 - accuracy: 0.9240
Epoch 00033: loss improved from 0.44793 to 0.44329, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4433 - accuracy: 0.9240 - val_loss: 0.
4556 - val_accuracy: 0.9137
Epoch 34/50
122/122 [==============================] - ETA: 0s - loss: 0.4396 - accuracy: 0.9244
Epoch 00034: loss improved from 0.44329 to 0.43965, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4396 - accuracy: 0.9244 - val_loss: 0.
4466 - val_accuracy: 0.9138
Epoch 35/50
122/122 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.9250
Epoch 00035: loss improved from 0.43965 to 0.43566, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 556ms/step - loss: 0.4357 - accuracy: 0.9250 - val_loss: 0.
4451 - val_accuracy: 0.9134
Epoch 36/50
122/122 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.9237
Epoch 00036: loss improved from 0.43566 to 0.43446, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 70s 571ms/step - loss: 0.4345 - accuracy: 0.9237 - val_loss: 0.
4375 - val_accuracy: 0.9193
Epoch 37/50
122/122 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.9266
Epoch 00037: loss improved from 0.43446 to 0.42841, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 71s 582ms/step - loss: 0.4284 - accuracy: 0.9266 - val_loss: 0.
4391 - val_accuracy: 0.9127
Epoch 38/50
122/122 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.9274
Epoch 00038: loss improved from 0.42841 to 0.42460, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 70s 577ms/step - loss: 0.4246 - accuracy: 0.9274 - val_loss: 0.
4300 - val_accuracy: 0.9206
Epoch 39/50
122/122 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.9272
Epoch 00039: loss improved from 0.42460 to 0.42199, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 557ms/step - loss: 0.4220 - accuracy: 0.9272 - val_loss: 0.
4331 - val_accuracy: 0.9118
Epoch 40/50
122/122 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.9278
Epoch 00040: loss improved from 0.42199 to 0.41818, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.4182 - accuracy: 0.9278 - val_loss: 0.
4267 - val_accuracy: 0.9185
Epoch 41/50
122/122 [==============================] - ETA: 0s - loss: 0.4150 - accuracy: 0.9282
Epoch 00041: loss improved from 0.41818 to 0.41495, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 555ms/step - loss: 0.4150 - accuracy: 0.9282 - val_loss: 0.
4250 - val_accuracy: 0.9178
Epoch 42/50
122/122 [==============================] - ETA: 0s - loss: 0.4126 - accuracy: 0.9285
Epoch 00042: loss improved from 0.41495 to 0.41260, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 553ms/step - loss: 0.4126 - accuracy: 0.9285 - val_loss: 0.
4187 - val_accuracy: 0.9213
Epoch 43/50
122/122 [==============================] - ETA: 0s - loss: 0.4079 - accuracy: 0.9300
Epoch 00043: loss improved from 0.41260 to 0.40789, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 551ms/step - loss: 0.4079 - accuracy: 0.9300 - val_loss: 0.
4194 - val_accuracy: 0.9171
Epoch 44/50
122/122 [==============================] - ETA: 0s - loss: 0.4047 - accuracy: 0.9303
Epoch 00044: loss improved from 0.40789 to 0.40470, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 68s 554ms/step - loss: 0.4047 - accuracy: 0.9303 - val_loss: 0.
4181 - val_accuracy: 0.9154
Epoch 45/50
122/122 [==============================] - ETA: 0s - loss: 0.4016 - accuracy: 0.9310
Epoch 00045: loss improved from 0.40470 to 0.40163, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 552ms/step - loss: 0.4016 - accuracy: 0.9310 - val_loss: 0.
4164 - val_accuracy: 0.9164
Epoch 46/50
122/122 [==============================] - ETA: 0s - loss: 0.3981 - accuracy: 0.9312
Epoch 00046: loss improved from 0.40163 to 0.39811, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.3981 - accuracy: 0.9312 - val_loss: 0.
4056 - val_accuracy: 0.9229
Epoch 47/50
122/122 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.9322
Epoch 00047: loss improved from 0.39811 to 0.39491, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3949 - accuracy: 0.9322 - val_loss: 0.
4054 - val_accuracy: 0.9205
Epoch 48/50
122/122 [==============================] - ETA: 0s - loss: 0.3916 - accuracy: 0.9325
Epoch 00048: loss improved from 0.39491 to 0.39161, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.3916 - accuracy: 0.9325 - val_loss: 0.
4085 - val_accuracy: 0.9167
Epoch 49/50
122/122 [==============================] - ETA: 0s - loss: 0.3886 - accuracy: 0.9327
Epoch 00049: loss improved from 0.39161 to 0.38862, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 551ms/step - loss: 0.3886 - accuracy: 0.9327 - val_loss: 0.
4024 - val_accuracy: 0.9205
Epoch 50/50
122/122 [==============================] - ETA: 0s - loss: 0.3859 - accuracy: 0.9328
Epoch 00050: loss improved from 0.38862 to 0.38588, saving model to ./weights\hs_v1_unet.hdf5
122/122 [==============================] - 67s 551ms/step - loss: 0.3859 - accuracy: 0.9328 - val_loss: 0.
4037 - val_accuracy: 0.9143
predict test data
test candidate data : (20, 512, 512, 1)
 1/20 [>.............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end`
 is slow compared to the batch time (batch time: 0.0020s vs `on_predict_batch_end` time: 0.0409s). Check y
our callbacks.
20/20 [==============================] - 1s 42ms/step
test result data : (20, 512, 512, 1)
array to image
PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master>