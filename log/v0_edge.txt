PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master> python hs_model_v0_e.py
2021-09-22 19:07:10.847883: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudart64_101.dll
(None, 512, 512, 1)
(None, 512, 512, 1)
2021-09-22 19:07:13.166801: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library nvcuda.dll
2021-09-22 19:07:13.223635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with pr
operties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-22 19:07:13.238629: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudart64_101.dll
2021-09-22 19:07:13.250862: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cublas64_10.dll
2021-09-22 19:07:13.262850: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cufft64_10.dll
2021-09-22 19:07:13.271125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library curand64_10.dll
2021-09-22 19:07:13.282287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusolver64_10.dll
2021-09-22 19:07:13.296988: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusparse64_10.dll
2021-09-22 19:07:13.322849: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudnn64_7.dll
2021-09-22 19:07:13.329832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu dev
ices: 0
2021-09-22 19:07:13.337973: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is
optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performa
nce-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-22 19:07:13.363003: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27412621eb0 ini
tialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-22 19:07:13.370801: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0):
Host, Default Version
2021-09-22 19:07:13.376230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with pr
operties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-22 19:07:13.388446: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudart64_101.dll
2021-09-22 19:07:13.397804: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cublas64_10.dll
2021-09-22 19:07:13.403757: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cufft64_10.dll
2021-09-22 19:07:13.411245: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library curand64_10.dll
2021-09-22 19:07:13.417388: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusolver64_10.dll
2021-09-22 19:07:13.427121: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusparse64_10.dll
2021-09-22 19:07:13.442873: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudnn64_7.dll
2021-09-22 19:07:13.457937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu dev
ices: 0
2021-09-22 19:07:14.031412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect St
reamExecutor with strength 1 edge matrix:
2021-09-22 19:07:14.038822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2021-09-22 19:07:14.043385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2021-09-22 19:07:14.048027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow dev
ice (/job:localhost/replica:0/task:0/device:GPU:0 with 8678 MB memory) -> physical GPU (device: 0, name: Ge
Force GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-09-22 19:07:14.084469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2743abe5220 ini
tialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-09-22 19:07:14.094478: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0):
GeForce GTX 1080 Ti, Compute Capability 6.1
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_2 (InputLayer)            [(None, 512, 512, 1) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 64) 640         input_2[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 512, 512, 64) 36928       conv2d[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 64) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 256, 256, 128 147584      conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 128, 128, 256 590080      conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 64, 512)  2359808     conv2d_6[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 64, 64, 512)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 512)  0           dropout[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 1024) 4719616     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 1024) 9438208     conv2d_8[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 1024) 0           conv2d_9[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 64, 64, 1024) 0           dropout_1[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 64, 64, 512)  2097664     up_sampling2d[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 64, 64, 1024) 0           dropout[0][0]
                                                                 conv2d_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_11[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_1[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 128, 512 0           conv2d_5[0][0]
                                                                 conv2d_13[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 256 590080      conv2d_14[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_15[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_2[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256, 256, 256 0           conv2d_3[0][0]
                                                                 conv2d_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 128 295040      concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 128 147584      conv2d_17[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_18[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_3[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 512, 512, 128 0           conv2d_1[0][0]
                                                                 conv2d_19[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 512, 512, 2)  1154        conv2d_21[0][0]
__________________________________________________________________________________________________
input_1 (InputLayer)            [(None, 512, 512, 1) 0
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 512, 512, 1)  3           conv2d_22[0][0]
==================================================================================================
Total params: 31,031,685
Trainable params: 31,031,685
Non-trainable params: 0
__________________________________________________________________________________________________
loading data
load train images...
------------------------------
load test images...
------------------------------
loading data done
(None, 512, 512, 1)
(None, 512, 512, 1)
got unet
Fitting model...
Epoch 1/50
2021-09-22 19:07:17.339457: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudnn64_7.dll
2021-09-22 19:07:18.071952: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking G
PU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-09-22 19:07:18.133070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cublas64_10.dll
  2/122 [..............................] - ETA: 33s - loss: 1.2735 - accuracy: 0.2132WARNING:tensorflow:Cal
lbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1695s vs `on_train_bat
ch_end` time: 0.3880s). Check your callbacks.
122/122 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.7322
Epoch 00001: loss improved from inf to 0.68912, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 69s 568ms/step - loss: 0.6891 - accuracy: 0.7322 - val_loss: 0.6
749 - val_accuracy: 0.7414
Epoch 2/50
122/122 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.7395
Epoch 00002: loss improved from 0.68912 to 0.67736, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 547ms/step - loss: 0.6774 - accuracy: 0.7395 - val_loss: 0.6
566 - val_accuracy: 0.7793
Epoch 3/50
122/122 [==============================] - ETA: 0s - loss: 0.6796 - accuracy: 0.7316
Epoch 00003: loss did not improve from 0.67736
122/122 [==============================] - 66s 540ms/step - loss: 0.6796 - accuracy: 0.7316 - val_loss: 0.6
768 - val_accuracy: 0.7254
Epoch 4/50
122/122 [==============================] - ETA: 0s - loss: 0.6805 - accuracy: 0.7232
Epoch 00004: loss did not improve from 0.67736
122/122 [==============================] - 66s 540ms/step - loss: 0.6805 - accuracy: 0.7232 - val_loss: 0.6
821 - val_accuracy: 0.7122
Epoch 5/50
122/122 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.7173
Epoch 00005: loss did not improve from 0.67736
122/122 [==============================] - 66s 540ms/step - loss: 0.6811 - accuracy: 0.7173 - val_loss: 0.6
798 - val_accuracy: 0.7122
Epoch 6/50
122/122 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.7173
Epoch 00006: loss did not improve from 0.67736
122/122 [==============================] - 66s 540ms/step - loss: 0.6785 - accuracy: 0.7173 - val_loss: 0.6
776 - val_accuracy: 0.7122
Epoch 7/50
122/122 [==============================] - ETA: 0s - loss: 0.6757 - accuracy: 0.7182
Epoch 00007: loss improved from 0.67736 to 0.67569, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 547ms/step - loss: 0.6757 - accuracy: 0.7182 - val_loss: 0.6
715 - val_accuracy: 0.7181
Epoch 8/50
122/122 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.7332
Epoch 00008: loss improved from 0.67569 to 0.66563, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 547ms/step - loss: 0.6656 - accuracy: 0.7332 - val_loss: 0.6
696 - val_accuracy: 0.7179
Epoch 9/50
122/122 [==============================] - ETA: 0s - loss: 0.6694 - accuracy: 0.7208
Epoch 00009: loss did not improve from 0.66563
122/122 [==============================] - 66s 540ms/step - loss: 0.6694 - accuracy: 0.7208 - val_loss: 0.6
697 - val_accuracy: 0.7142
Epoch 10/50
122/122 [==============================] - ETA: 0s - loss: 0.6666 - accuracy: 0.7218
Epoch 00010: loss did not improve from 0.66563
122/122 [==============================] - 66s 540ms/step - loss: 0.6666 - accuracy: 0.7218 - val_loss: 0.6
612 - val_accuracy: 0.7298
Epoch 11/50
122/122 [==============================] - ETA: 0s - loss: 0.6624 - accuracy: 0.7300
Epoch 00011: loss improved from 0.66563 to 0.66242, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 547ms/step - loss: 0.6624 - accuracy: 0.7300 - val_loss: 0.6
681 - val_accuracy: 0.7103
Epoch 12/50
122/122 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.7157
Epoch 00012: loss did not improve from 0.66242
122/122 [==============================] - 66s 539ms/step - loss: 0.6665 - accuracy: 0.7157 - val_loss: 0.6
647 - val_accuracy: 0.7122
Epoch 13/50
122/122 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.7173
Epoch 00013: loss did not improve from 0.66242
122/122 [==============================] - 66s 539ms/step - loss: 0.6632 - accuracy: 0.7173 - val_loss: 0.6
627 - val_accuracy: 0.7122
Epoch 14/50
122/122 [==============================] - ETA: 0s - loss: 0.6611 - accuracy: 0.7173
Epoch 00014: loss improved from 0.66242 to 0.66112, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 546ms/step - loss: 0.6611 - accuracy: 0.7173 - val_loss: 0.6
607 - val_accuracy: 0.7123
Epoch 15/50
122/122 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.7221
Epoch 00015: loss improved from 0.66112 to 0.65613, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 546ms/step - loss: 0.6561 - accuracy: 0.7221 - val_loss: 0.6
520 - val_accuracy: 0.7227
Epoch 16/50
122/122 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.7626
Epoch 00016: loss improved from 0.65613 to 0.63065, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 547ms/step - loss: 0.6306 - accuracy: 0.7626 - val_loss: 0.5
460 - val_accuracy: 0.8732
Epoch 17/50
122/122 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.8719
Epoch 00017: loss improved from 0.63065 to 0.55254, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.5525 - accuracy: 0.8719 - val_loss: 0.5
260 - val_accuracy: 0.8948
Epoch 18/50
122/122 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.9199
Epoch 00018: loss improved from 0.55254 to 0.50650, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.5065 - accuracy: 0.9199 - val_loss: 0.5
034 - val_accuracy: 0.9173
Epoch 19/50
122/122 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.9327
Epoch 00019: loss improved from 0.50650 to 0.48716, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4872 - accuracy: 0.9327 - val_loss: 0.5
099 - val_accuracy: 0.9045
Epoch 20/50
122/122 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.9358
Epoch 00020: loss improved from 0.48716 to 0.47954, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4795 - accuracy: 0.9358 - val_loss: 0.4
853 - val_accuracy: 0.9277
Epoch 21/50
122/122 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.9386
Epoch 00021: loss improved from 0.47954 to 0.47287, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4729 - accuracy: 0.9386 - val_loss: 0.4
822 - val_accuracy: 0.9231
Epoch 22/50
122/122 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.9386
Epoch 00022: loss improved from 0.47287 to 0.46863, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4686 - accuracy: 0.9386 - val_loss: 0.4
741 - val_accuracy: 0.9295
Epoch 23/50
122/122 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.9408
Epoch 00023: loss improved from 0.46863 to 0.46235, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4623 - accuracy: 0.9408 - val_loss: 0.4
736 - val_accuracy: 0.9277
Epoch 24/50
122/122 [==============================] - ETA: 0s - loss: 0.4575 - accuracy: 0.9419
Epoch 00024: loss improved from 0.46235 to 0.45745, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4575 - accuracy: 0.9419 - val_loss: 0.4
632 - val_accuracy: 0.9318
Epoch 25/50
122/122 [==============================] - ETA: 0s - loss: 0.4520 - accuracy: 0.9436
Epoch 00025: loss improved from 0.45745 to 0.45200, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4520 - accuracy: 0.9436 - val_loss: 0.4
582 - val_accuracy: 0.9336
Epoch 26/50
122/122 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.9439
Epoch 00026: loss improved from 0.45200 to 0.44745, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4475 - accuracy: 0.9439 - val_loss: 0.4
568 - val_accuracy: 0.9329
Epoch 27/50
122/122 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.9458
Epoch 00027: loss improved from 0.44745 to 0.44223, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4422 - accuracy: 0.9458 - val_loss: 0.4
517 - val_accuracy: 0.9345
Epoch 28/50
122/122 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.9465
Epoch 00028: loss improved from 0.44223 to 0.43795, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4380 - accuracy: 0.9465 - val_loss: 0.4
456 - val_accuracy: 0.9372
Epoch 29/50
122/122 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.9450
Epoch 00029: loss improved from 0.43795 to 0.43560, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4356 - accuracy: 0.9450 - val_loss: 0.4
429 - val_accuracy: 0.9356
Epoch 30/50
122/122 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.9472
Epoch 00030: loss improved from 0.43560 to 0.43018, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4302 - accuracy: 0.9472 - val_loss: 0.4
373 - val_accuracy: 0.9368
Epoch 31/50
122/122 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.9481
Epoch 00031: loss improved from 0.43018 to 0.42587, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4259 - accuracy: 0.9481 - val_loss: 0.4
350 - val_accuracy: 0.9363
Epoch 32/50
122/122 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.9496
Epoch 00032: loss improved from 0.42587 to 0.42125, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4213 - accuracy: 0.9496 - val_loss: 0.4
327 - val_accuracy: 0.9370
Epoch 33/50
122/122 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.9512
Epoch 00033: loss improved from 0.42125 to 0.41599, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4160 - accuracy: 0.9512 - val_loss: 0.4
287 - val_accuracy: 0.9389
Epoch 34/50
122/122 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.9497
Epoch 00034: loss improved from 0.41599 to 0.41459, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4146 - accuracy: 0.9497 - val_loss: 0.4
273 - val_accuracy: 0.9390
Epoch 35/50
122/122 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.9519
Epoch 00035: loss improved from 0.41459 to 0.40848, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4085 - accuracy: 0.9519 - val_loss: 0.4
287 - val_accuracy: 0.9387
Epoch 36/50
122/122 [==============================] - ETA: 0s - loss: 0.4052 - accuracy: 0.9522
Epoch 00036: loss improved from 0.40848 to 0.40518, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4052 - accuracy: 0.9522 - val_loss: 0.4
378 - val_accuracy: 0.9273
Epoch 37/50
122/122 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.9535
Epoch 00037: loss improved from 0.40518 to 0.40049, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4005 - accuracy: 0.9535 - val_loss: 0.4
143 - val_accuracy: 0.9398
Epoch 38/50
122/122 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.9536
Epoch 00038: loss improved from 0.40049 to 0.39700, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3970 - accuracy: 0.9536 - val_loss: 0.4
065 - val_accuracy: 0.9453
Epoch 39/50
122/122 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.9552
Epoch 00039: loss improved from 0.39700 to 0.39220, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.3922 - accuracy: 0.9552 - val_loss: 0.4
039 - val_accuracy: 0.9446
Epoch 40/50
122/122 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.9560
Epoch 00040: loss improved from 0.39220 to 0.38818, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3882 - accuracy: 0.9560 - val_loss: 0.4
099 - val_accuracy: 0.9384
Epoch 41/50
122/122 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.9558
Epoch 00041: loss improved from 0.38818 to 0.38510, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3851 - accuracy: 0.9558 - val_loss: 0.4
036 - val_accuracy: 0.9459
Epoch 42/50
122/122 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.9570
Epoch 00042: loss improved from 0.38510 to 0.38091, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3809 - accuracy: 0.9570 - val_loss: 0.4
044 - val_accuracy: 0.9387
Epoch 43/50
122/122 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.9574
Epoch 00043: loss improved from 0.38091 to 0.37748, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.3775 - accuracy: 0.9574 - val_loss: 0.3
934 - val_accuracy: 0.9426
Epoch 44/50
122/122 [==============================] - ETA: 0s - loss: 0.3753 - accuracy: 0.9571
Epoch 00044: loss improved from 0.37748 to 0.37526, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3753 - accuracy: 0.9571 - val_loss: 0.3
849 - val_accuracy: 0.9477
Epoch 45/50
122/122 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.9593
Epoch 00045: loss improved from 0.37526 to 0.36910, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.3691 - accuracy: 0.9593 - val_loss: 0.3
873 - val_accuracy: 0.9486
Epoch 46/50
122/122 [==============================] - ETA: 0s - loss: 0.3651 - accuracy: 0.9603
Epoch 00046: loss improved from 0.36910 to 0.36510, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3651 - accuracy: 0.9603 - val_loss: 0.3
811 - val_accuracy: 0.9471
Epoch 47/50
122/122 [==============================] - ETA: 0s - loss: 0.3610 - accuracy: 0.9614
Epoch 00047: loss improved from 0.36510 to 0.36102, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3610 - accuracy: 0.9614 - val_loss: 0.3
765 - val_accuracy: 0.9493
Epoch 48/50
122/122 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.9607
Epoch 00048: loss improved from 0.36102 to 0.35939, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.3594 - accuracy: 0.9607 - val_loss: 0.3
761 - val_accuracy: 0.9474
Epoch 49/50
122/122 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.9579
Epoch 00049: loss did not improve from 0.35939
122/122 [==============================] - 66s 541ms/step - loss: 0.3603 - accuracy: 0.9579 - val_loss: 0.3
713 - val_accuracy: 0.9466
Epoch 50/50
122/122 [==============================] - ETA: 0s - loss: 0.3522 - accuracy: 0.9617
Epoch 00050: loss improved from 0.35939 to 0.35216, saving model to ./weights\hs_v0_e_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.3522 - accuracy: 0.9617 - val_loss: 0.3
813 - val_accuracy: 0.9443
predict test data
test candidate data : (20, 512, 512, 1)
 1/20 [>.............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end`
is slow compared to the batch time (batch time: 0.0020s vs `on_predict_batch_end` time: 0.0409s). Check you
r callbacks.
20/20 [==============================] - 1s 42ms/step
test result data : (20, 512, 512, 1)
array to image
PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master>