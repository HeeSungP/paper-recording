PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master> python hs_model_v1.py
2021-09-22 12:04:46.863109: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudart64_101.dll
(None, 512, 512, 1)
(None, 512, 512, 1)
2021-09-22 12:04:49.194258: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library nvcuda.dll
2021-09-22 12:04:49.237026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with pr
operties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-22 12:04:49.249962: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudart64_101.dll
2021-09-22 12:04:49.260263: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cublas64_10.dll
2021-09-22 12:04:49.270195: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cufft64_10.dll
2021-09-22 12:04:49.278157: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library curand64_10.dll
2021-09-22 12:04:49.299427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusolver64_10.dll
2021-09-22 12:04:49.310077: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusparse64_10.dll
2021-09-22 12:04:49.326025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudnn64_7.dll
2021-09-22 12:04:49.332310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu dev
ices: 0
2021-09-22 12:04:49.340963: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is
optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performa
nce-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-22 12:04:49.362868: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1bc14543ce0 ini
tialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-22 12:04:49.374007: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0):
Host, Default Version
2021-09-22 12:04:49.379637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with pr
operties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-22 12:04:49.394305: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudart64_101.dll
2021-09-22 12:04:49.401132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cublas64_10.dll
2021-09-22 12:04:49.407102: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cufft64_10.dll
2021-09-22 12:04:49.425912: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library curand64_10.dll
2021-09-22 12:04:49.432916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusolver64_10.dll
2021-09-22 12:04:49.439111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cusparse64_10.dll
2021-09-22 12:04:49.445378: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudnn64_7.dll
2021-09-22 12:04:49.454620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu dev
ices: 0
2021-09-22 12:04:50.038532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect St
reamExecutor with strength 1 edge matrix:
2021-09-22 12:04:50.045563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2021-09-22 12:04:50.051362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2021-09-22 12:04:50.055381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow dev
ice (/job:localhost/replica:0/task:0/device:GPU:0 with 8678 MB memory) -> physical GPU (device: 0, name: Ge
Force GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-09-22 12:04:50.084126: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1bc4a4c7710 ini
tialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-09-22 12:04:50.093185: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0):
GeForce GTX 1080 Ti, Compute Capability 6.1
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 512, 512, 1) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 64) 640         input_1[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 512, 512, 64) 36928       conv2d[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 64) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 256, 256, 128 147584      conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 128, 128, 256 590080      conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 64, 512)  2359808     conv2d_6[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 64, 64, 512)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 512)  0           dropout[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 1024) 4719616     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 1024) 9438208     conv2d_8[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 1024) 0           conv2d_9[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 64, 64, 1024) 0           dropout_1[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 64, 64, 512)  2097664     up_sampling2d[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 64, 64, 1024) 0           dropout[0][0]
                                                                 conv2d_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_11[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_1[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 128, 512 0           conv2d_5[0][0]
                                                                 conv2d_13[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 256 590080      conv2d_14[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_15[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_2[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256, 256, 256 0           conv2d_3[0][0]
                                                                 conv2d_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 128 295040      concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 128 147584      conv2d_17[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_18[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_3[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 512, 512, 128 0           conv2d_1[0][0]
                                                                 conv2d_19[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 512, 512, 2)  1154        conv2d_21[0][0]
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 512, 512, 1) 0
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 512, 512, 3)  0           conv2d_22[0][0]
                                                                 input_2[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 512, 512, 2)  56          concatenate_4[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 512, 512, 1)  19          conv2d_23[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 512, 512, 1)  2           conv2d_24[0][0]
==================================================================================================
Total params: 31,031,759
Trainable params: 31,031,759
Non-trainable params: 0
__________________________________________________________________________________________________
loading data
load train images...
------------------------------
load test images...
------------------------------
loading data done
(None, 512, 512, 1)
(None, 512, 512, 1)
got unet
Fitting model...
Epoch 1/50
2021-09-22 12:04:52.631491: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cudnn64_7.dll
2021-09-22 12:04:53.371140: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking G
PU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-09-22 12:04:53.421554: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully op
ened dynamic library cublas64_10.dll
  2/162 [..............................] - ETA: 12s - loss: 0.6655 - accuracy: 0.6802WARNING:tensorflow:Cal
lbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_bat
ch_end` time: 0.1416s). Check your callbacks.
162/162 [==============================] - ETA: 0s - loss: 0.3984 - accuracy: 0.8230WARNING:tensorflow:Call
backs method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_test_batch_
end` time: 0.0399s). Check your callbacks.

Epoch 00001: loss improved from inf to 0.39839, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 161ms/step - loss: 0.3984 - accuracy: 0.8230 - val_loss: 0.3
491 - val_accuracy: 0.8924
Epoch 2/50
162/162 [==============================] - ETA: 0s - loss: 0.3034 - accuracy: 0.9143
Epoch 00002: loss improved from 0.39839 to 0.30338, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.3034 - accuracy: 0.9143 - val_loss: 0.3
390 - val_accuracy: 0.8993
Epoch 3/50
162/162 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.9230
Epoch 00003: loss improved from 0.30338 to 0.29382, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 162ms/step - loss: 0.2938 - accuracy: 0.9230 - val_loss: 0.3
285 - val_accuracy: 0.9078
Epoch 4/50
162/162 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9286
Epoch 00004: loss improved from 0.29382 to 0.28979, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 162ms/step - loss: 0.2898 - accuracy: 0.9286 - val_loss: 0.3
255 - val_accuracy: 0.9129
Epoch 5/50
162/162 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9315
Epoch 00005: loss improved from 0.28979 to 0.27849, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2785 - accuracy: 0.9315 - val_loss: 0.3
224 - val_accuracy: 0.9059
Epoch 6/50
162/162 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.9344
Epoch 00006: loss improved from 0.27849 to 0.27371, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2737 - accuracy: 0.9344 - val_loss: 0.3
191 - val_accuracy: 0.9222
Epoch 7/50
162/162 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9391
Epoch 00007: loss improved from 0.27371 to 0.26693, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 165ms/step - loss: 0.2669 - accuracy: 0.9391 - val_loss: 0.3
254 - val_accuracy: 0.9125
Epoch 8/50
162/162 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9405
Epoch 00008: loss improved from 0.26693 to 0.26611, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2661 - accuracy: 0.9405 - val_loss: 0.3
067 - val_accuracy: 0.9207
Epoch 9/50
162/162 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.9345
Epoch 00009: loss did not improve from 0.26611
162/162 [==============================] - 25s 157ms/step - loss: 0.2679 - accuracy: 0.9345 - val_loss: 0.3
072 - val_accuracy: 0.9245
Epoch 10/50
162/162 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9399
Epoch 00010: loss improved from 0.26611 to 0.26085, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.2608 - accuracy: 0.9399 - val_loss: 0.3
065 - val_accuracy: 0.9172
Epoch 11/50
162/162 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9403
Epoch 00011: loss improved from 0.26085 to 0.25710, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2571 - accuracy: 0.9403 - val_loss: 0.3
071 - val_accuracy: 0.9129
Epoch 12/50
162/162 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9420
Epoch 00012: loss improved from 0.25710 to 0.25557, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.2556 - accuracy: 0.9420 - val_loss: 0.3
182 - val_accuracy: 0.9215
Epoch 13/50
162/162 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.9420
Epoch 00013: loss improved from 0.25557 to 0.25215, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 162ms/step - loss: 0.2522 - accuracy: 0.9420 - val_loss: 0.3
083 - val_accuracy: 0.9154
Epoch 14/50
162/162 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.9436
Epoch 00014: loss improved from 0.25215 to 0.25032, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2503 - accuracy: 0.9436 - val_loss: 0.2
966 - val_accuracy: 0.9274
Epoch 15/50
162/162 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9450
Epoch 00015: loss improved from 0.25032 to 0.24684, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.2468 - accuracy: 0.9450 - val_loss: 0.2
908 - val_accuracy: 0.9274
Epoch 16/50
162/162 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9451
Epoch 00016: loss improved from 0.24684 to 0.24401, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 162ms/step - loss: 0.2440 - accuracy: 0.9451 - val_loss: 0.2
994 - val_accuracy: 0.9253
Epoch 17/50
162/162 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9457
Epoch 00017: loss improved from 0.24401 to 0.24014, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 167ms/step - loss: 0.2401 - accuracy: 0.9457 - val_loss: 0.3
000 - val_accuracy: 0.9265
Epoch 18/50
162/162 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9469
Epoch 00018: loss did not improve from 0.24014
162/162 [==============================] - 26s 162ms/step - loss: 0.2405 - accuracy: 0.9469 - val_loss: 0.2
911 - val_accuracy: 0.9273
Epoch 19/50
162/162 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9476
Epoch 00019: loss improved from 0.24014 to 0.23488, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 167ms/step - loss: 0.2349 - accuracy: 0.9476 - val_loss: 0.2
914 - val_accuracy: 0.9300
Epoch 20/50
162/162 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9496
Epoch 00020: loss improved from 0.23488 to 0.23129, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 169ms/step - loss: 0.2313 - accuracy: 0.9496 - val_loss: 0.2
862 - val_accuracy: 0.9281
Epoch 21/50
162/162 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9487
Epoch 00021: loss did not improve from 0.23129
162/162 [==============================] - 26s 162ms/step - loss: 0.2323 - accuracy: 0.9487 - val_loss: 0.2
953 - val_accuracy: 0.9300
Epoch 22/50
162/162 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.9503
Epoch 00022: loss improved from 0.23129 to 0.22727, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 165ms/step - loss: 0.2273 - accuracy: 0.9503 - val_loss: 0.2
831 - val_accuracy: 0.9280
Epoch 23/50
162/162 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9530
Epoch 00023: loss improved from 0.22727 to 0.22203, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2220 - accuracy: 0.9530 - val_loss: 0.2
772 - val_accuracy: 0.9309
Epoch 24/50
162/162 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9516
Epoch 00024: loss did not improve from 0.22203
162/162 [==============================] - 26s 158ms/step - loss: 0.2222 - accuracy: 0.9516 - val_loss: 0.2
929 - val_accuracy: 0.9182
Epoch 25/50
162/162 [==============================] - ETA: 0s - loss: 0.2191 - accuracy: 0.9532
Epoch 00025: loss improved from 0.22203 to 0.21912, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2191 - accuracy: 0.9532 - val_loss: 0.2
744 - val_accuracy: 0.9315
Epoch 26/50
162/162 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9542
Epoch 00026: loss improved from 0.21912 to 0.21476, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 166ms/step - loss: 0.2148 - accuracy: 0.9542 - val_loss: 0.2
800 - val_accuracy: 0.9308
Epoch 27/50
162/162 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9552
Epoch 00027: loss improved from 0.21476 to 0.21038, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.2104 - accuracy: 0.9552 - val_loss: 0.2
775 - val_accuracy: 0.9321
Epoch 28/50
162/162 [==============================] - ETA: 0s - loss: 0.2081 - accuracy: 0.9556
Epoch 00028: loss improved from 0.21038 to 0.20814, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 162ms/step - loss: 0.2081 - accuracy: 0.9556 - val_loss: 0.2
879 - val_accuracy: 0.9313
Epoch 29/50
162/162 [==============================] - ETA: 0s - loss: 0.2038 - accuracy: 0.9584
Epoch 00029: loss improved from 0.20814 to 0.20379, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.2038 - accuracy: 0.9584 - val_loss: 0.2
775 - val_accuracy: 0.9307
Epoch 30/50
162/162 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9580
Epoch 00030: loss improved from 0.20379 to 0.20216, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 164ms/step - loss: 0.2022 - accuracy: 0.9580 - val_loss: 0.2
671 - val_accuracy: 0.9328
Epoch 31/50
162/162 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9592
Epoch 00031: loss improved from 0.20216 to 0.19812, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1981 - accuracy: 0.9592 - val_loss: 0.2
724 - val_accuracy: 0.9309
Epoch 32/50
162/162 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9598
Epoch 00032: loss improved from 0.19812 to 0.19549, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1955 - accuracy: 0.9598 - val_loss: 0.2
689 - val_accuracy: 0.9334
Epoch 33/50
162/162 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9584
Epoch 00033: loss did not improve from 0.19549
162/162 [==============================] - 25s 157ms/step - loss: 0.1963 - accuracy: 0.9584 - val_loss: 0.2
792 - val_accuracy: 0.9330
Epoch 34/50
162/162 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9607
Epoch 00034: loss improved from 0.19549 to 0.19160, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1916 - accuracy: 0.9607 - val_loss: 0.2
781 - val_accuracy: 0.9273
Epoch 35/50
162/162 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9609
Epoch 00035: loss improved from 0.19160 to 0.18889, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1889 - accuracy: 0.9609 - val_loss: 0.2
599 - val_accuracy: 0.9329
Epoch 36/50
162/162 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.9624
Epoch 00036: loss improved from 0.18889 to 0.18546, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.1855 - accuracy: 0.9624 - val_loss: 0.2
820 - val_accuracy: 0.9343
Epoch 37/50
162/162 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9631
Epoch 00037: loss improved from 0.18546 to 0.18273, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 166ms/step - loss: 0.1827 - accuracy: 0.9631 - val_loss: 0.2
825 - val_accuracy: 0.9322
Epoch 38/50
162/162 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9643
Epoch 00038: loss improved from 0.18273 to 0.17782, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 165ms/step - loss: 0.1778 - accuracy: 0.9643 - val_loss: 0.2
905 - val_accuracy: 0.9325
Epoch 39/50
162/162 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.9642
Epoch 00039: loss improved from 0.17782 to 0.17639, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1764 - accuracy: 0.9642 - val_loss: 0.2
757 - val_accuracy: 0.9271
Epoch 40/50
162/162 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9649
Epoch 00040: loss improved from 0.17639 to 0.17312, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 166ms/step - loss: 0.1731 - accuracy: 0.9649 - val_loss: 0.3
034 - val_accuracy: 0.9328
Epoch 41/50
162/162 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9650
Epoch 00041: loss improved from 0.17312 to 0.17261, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 27s 164ms/step - loss: 0.1726 - accuracy: 0.9650 - val_loss: 0.2
794 - val_accuracy: 0.9294
Epoch 42/50
162/162 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.9605
Epoch 00042: loss did not improve from 0.17261
162/162 [==============================] - 26s 159ms/step - loss: 0.1803 - accuracy: 0.9605 - val_loss: 0.2
992 - val_accuracy: 0.9320
Epoch 43/50
162/162 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9648
Epoch 00043: loss improved from 0.17261 to 0.16968, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1697 - accuracy: 0.9648 - val_loss: 0.2
731 - val_accuracy: 0.9339
Epoch 44/50
162/162 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9668
Epoch 00044: loss improved from 0.16968 to 0.16486, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1649 - accuracy: 0.9668 - val_loss: 0.2
617 - val_accuracy: 0.9309
Epoch 45/50
162/162 [==============================] - ETA: 0s - loss: 0.1603 - accuracy: 0.9680
Epoch 00045: loss improved from 0.16486 to 0.16033, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1603 - accuracy: 0.9680 - val_loss: 0.2
958 - val_accuracy: 0.9311
Epoch 46/50
162/162 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9682
Epoch 00046: loss improved from 0.16033 to 0.15933, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1593 - accuracy: 0.9682 - val_loss: 0.2
762 - val_accuracy: 0.9277
Epoch 47/50
162/162 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.9694
Epoch 00047: loss improved from 0.15933 to 0.15625, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1563 - accuracy: 0.9694 - val_loss: 0.2
803 - val_accuracy: 0.9274
Epoch 48/50
162/162 [==============================] - ETA: 0s - loss: 0.1544 - accuracy: 0.9696
Epoch 00048: loss improved from 0.15625 to 0.15441, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1544 - accuracy: 0.9696 - val_loss: 0.2
741 - val_accuracy: 0.9313
Epoch 49/50
162/162 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9701
Epoch 00049: loss improved from 0.15441 to 0.15186, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 162ms/step - loss: 0.1519 - accuracy: 0.9701 - val_loss: 0.3
190 - val_accuracy: 0.9290
Epoch 50/50
162/162 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9707
Epoch 00050: loss improved from 0.15186 to 0.15017, saving model to ./weights\hs_v1_unet.hdf5
162/162 [==============================] - 26s 163ms/step - loss: 0.1502 - accuracy: 0.9707 - val_loss: 0.2
823 - val_accuracy: 0.9339
predict test data
test candidate data : (20, 512, 512, 1)
 1/20 [>.............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end`
is slow compared to the batch time (batch time: 0.0030s vs `on_predict_batch_end` time: 0.0409s). Check you
r callbacks.
20/20 [==============================] - 1s 44ms/step
test result data : (20, 512, 512, 1)
array to image
PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master>