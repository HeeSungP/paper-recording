PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master> python hs_model_v4_2.py
2021-09-22 23:21:28.200445: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cudart64_101.dll
Creating training images...
540 540
C:\Users\HEESUNG\anaconda3\envs\tf_keras\lib\site-packages\keras_preprocessing\image\utils.py:107: UserWarning: gr
ayscale is deprecated. Please use color_mode = "grayscale"
  warnings.warn('grayscale is deprecated. Please use '
Done: 0/540 images
Done: 60/540 images
Done: 120/540 images
Done: 180/540 images
Done: 240/540 images
Done: 300/540 images
Done: 360/540 images
Done: 420/540 images
Done: 480/540 images
loading done
Saving to .npy files done.
Creating test images...
loading done
Saving to imgs_test.npy files done.
(None, 512, 512, 2)
(None, 512, 512, 1)
2021-09-22 23:21:42.827422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library nvcuda.dll
2021-09-22 23:21:42.879138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with propertie
s:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-22 23:21:42.901885: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cudart64_101.dll
2021-09-22 23:21:42.916992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cublas64_10.dll
2021-09-22 23:21:42.927125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cufft64_10.dll
2021-09-22 23:21:42.935092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library curand64_10.dll
2021-09-22 23:21:42.947740: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cusolver64_10.dll
2021-09-22 23:21:42.958833: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cusparse64_10.dll
2021-09-22 23:21:42.975354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cudnn64_7.dll
2021-09-22 23:21:42.981178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0

2021-09-22 23:21:42.986634: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimiz
ed with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical o
perations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-22 23:21:43.008549: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27df913c240 initialize
d for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-22 23:21:43.016639: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, D
efault Version
2021-09-22 23:21:43.044214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with propertie
s:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2021-09-22 23:21:43.064680: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cudart64_101.dll
2021-09-22 23:21:43.074582: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cublas64_10.dll
2021-09-22 23:21:43.082682: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cufft64_10.dll
2021-09-22 23:21:43.090416: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library curand64_10.dll
2021-09-22 23:21:43.099231: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cusolver64_10.dll
2021-09-22 23:21:43.105571: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cusparse64_10.dll
2021-09-22 23:21:43.113181: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dy
namic library cudnn64_7.dll
2021-09-22 23:21:43.121873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0

2021-09-22 23:21:43.705524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExe
cutor with strength 1 edge matrix:
2021-09-22 23:21:43.713431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2021-09-22 23:21:43.718063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2021-09-22 23:21:43.725694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/j
ob:localhost/replica:0/task:0/device:GPU:0 with 8678 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080
 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-09-22 23:21:43.742396: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27da2c1a110 initialize
d for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-09-22 23:21:43.753355: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce
 GTX 1080 Ti, Compute Capability 6.1
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 512, 512, 2) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 512, 512, 64) 1216        input_1[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 512, 512, 64) 36928       conv2d[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 256, 256, 64) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 256, 256, 128 147584      conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 128, 128, 256 590080      conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 64, 64, 512)  2359808     conv2d_6[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 64, 64, 512)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 512)  0           dropout[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 1024) 4719616     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 1024) 9438208     conv2d_8[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 1024) 0           conv2d_9[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 64, 64, 1024) 0           dropout_1[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 64, 64, 512)  2097664     up_sampling2d[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 64, 64, 1024) 0           dropout[0][0]
                                                                 conv2d_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_11[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_1[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 128, 512 0           conv2d_5[0][0]
                                                                 conv2d_13[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 256 590080      conv2d_14[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_15[0][0]
__________________________________________________________________________________________________
















                conv2d_16 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_2[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256, 256, 256 0           conv2d_3[0][0]
                                                                 conv2d_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 128 295040      concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 128 147584      conv2d_17[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_18[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_3[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 512, 512, 128 0           conv2d_1[0][0]
                                                                 conv2d_19[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 512, 512, 2)  1154        conv2d_21[0][0]
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 512, 512, 1) 0
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 512, 512, 3)  0           conv2d_22[0][0]
                                                                 input_2[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 512, 512, 2)  56          concatenate_4[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 512, 512, 1)  19          conv2d_23[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 512, 512, 1)  2           conv2d_24[0][0]
namic library cudnn64_7.dll
2021-09-22 23:21:48.331611: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm
compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation.
namic library cudnn64_7.dll
2021-09-22 23:21:48.331611: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm
compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation.


                                                                                                             ed dy

                                                                                                             nsorf
                                                                                                             n_bat

                                                                                                             23:22
                                                                                                             ying
                                                                                                              ther

                                                                                                             out o
                                                                                                             ut ma



                                                                                                             al_lo

                                                     Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-09-22 23:21:48.397784: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully openal_lo
ed dynamic library cublas64_10.dll
  2/122 [..............................] - ETA: 31s - loss: 0.6981 - accuracy: 0.5800 - mse: 0.2524WARNING:te
nsorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1685s vs `on
_train_batch_end` time: 0.3560s). Check your callbacks.
121/122 [============================>.] - ETA: 0s - loss: 0.6875 - accuracy: 0.7150 - mse: 0.24752021-09-22 al_lo
23:22:59.116396: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memo
ry trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but may
 mean that there could be performance gains if more memory were available.
2021-09-22 23:23:00.148326: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran
out of memory trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failual_lo
re, but may mean that there could be performance gains if more memory were available.
122/122 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.7151 - mse: 0.2475
Epoch 00001: loss improved from inf to 0.68745, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 72s 587ms/step - loss: 0.6874 - accuracy: 0.7151 - mse: 0.2475 - v
al_loss: 0.6832 - val_accuracy: 0.7235 - val_mse: 0.2457                                                     al_lo
Epoch 2/50
122/122 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.7097 - mse: 0.2464
Epoch 00002: loss improved from 0.68745 to 0.68497, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 68s 557ms/step - loss: 0.6850 - accuracy: 0.7097 - mse: 0.2464 - v
al_loss: 0.6883 - val_accuracy: 0.6624 - val_mse: 0.2477                                                     al_lo
Epoch 3/50
122/122 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.6790 - mse: 0.2468
Epoch 00003: loss did not improve from 0.68497
122/122 [==============================] - 66s 543ms/step - loss: 0.6863 - accuracy: 0.6790 - mse: 0.2468 - v
al_loss: 0.6847 - val_accuracy: 0.6872 - val_mse: 0.2460                                                     al_lo
Epoch 4/50
122/122 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.7122 - mse: 0.2439
Epoch 00004: loss improved from 0.68497 to 0.68031, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.6803 - accuracy: 0.7122 - mse: 0.2439 - v
al_loss: 0.6673 - val_accuracy: 0.7388 - val_mse: 0.2383                                                     al_lo
Epoch 5/50
122/122 [==============================] - ETA: 0s - loss: 0.6569 - accuracy: 0.7564 - mse: 0.2342
Epoch 00005: loss improved from 0.68031 to 0.65690, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 551ms/step - loss: 0.6569 - accuracy: 0.7564 - mse: 0.2342 - v
al_loss: 0.6635 - val_accuracy: 0.7561 - val_mse: 0.2345                                                     al_lo
Epoch 6/50
122/122 [==============================] - ETA: 0s - loss: 0.6416 - accuracy: 0.7755 - mse: 0.2277
Epoch 00006: loss improved from 0.65690 to 0.64163, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 69s 564ms/step - loss: 0.6416 - accuracy: 0.7755 - mse: 0.2277 - v
al_loss: 0.6507 - val_accuracy: 0.7627 - val_mse: 0.2296
Epoch 7/50
122/122 [==============================] - ETA: 0s - loss: 0.6269 - accuracy: 0.7907 - mse: 0.2219
Epoch 00007: loss improved from 0.64163 to 0.62695, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.6269 - accuracy: 0.7907 - mse: 0.2219 - v
al_loss: 0.6457 - val_accuracy: 0.7589 - val_mse: 0.2288
Epoch 8/50
122/122 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.7844 - mse: 0.2225
Epoch 00008: loss did not improve from 0.62695
122/122 [==============================] - 66s 545ms/step - loss: 0.6296 - accuracy: 0.7844 - mse: 0.2225 - v
al_loss: 0.6345 - val_accuracy: 0.7772 - val_mse: 0.2226
Epoch 9/50
122/122 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.8064 - mse: 0.2152
Epoch 00009: loss improved from 0.62695 to 0.61251, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.6125 - accuracy: 0.8064 - mse: 0.2152 - v
al_loss: 0.6187 - val_accuracy: 0.8041 - val_mse: 0.2148
Epoch 10/50
122/122 [==============================] - ETA: 0s - loss: 0.5764 - accuracy: 0.8579 - mse: 0.2000
Epoch 00010: loss improved from 0.61251 to 0.57636, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 69s 563ms/step - loss: 0.5764 - accuracy: 0.8579 - mse: 0.2000 - v
al_loss: 0.5680 - val_accuracy: 0.8748 - val_mse: 0.1948
Epoch 11/50
122/122 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.9202 - mse: 0.1801
Epoch 00011: loss improved from 0.57636 to 0.52351, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.5235 - accuracy: 0.9202 - mse: 0.1801 - v
al_loss: 0.5399 - val_accuracy: 0.9297 - val_mse: 0.1784
Epoch 12/50
122/122 [==============================] - ETA: 0s - loss: 0.4880 - accuracy: 0.9516 - mse: 0.1678
Epoch 00012: loss improved from 0.52351 to 0.48804, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.4880 - accuracy: 0.9516 - mse: 0.1678 - v
al_loss: 0.5082 - val_accuracy: 0.9354 - val_mse: 0.1721
Epoch 13/50
122/122 [==============================] - ETA: 0s - loss: 0.4764 - accuracy: 0.9584 - mse: 0.1632
Epoch 00013: loss improved from 0.48804 to 0.47637, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4764 - accuracy: 0.9584 - mse: 0.1632 - v
al_loss: 0.5063 - val_accuracy: 0.9380 - val_mse: 0.1705
Epoch 14/50
122/122 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.9673 - mse: 0.1583
Epoch 00014: loss improved from 0.47637 to 0.46446, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4645 - accuracy: 0.9673 - mse: 0.1583 - v
al_loss: 0.4923 - val_accuracy: 0.9479 - val_mse: 0.1646
Epoch 15/50
122/122 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.9684 - mse: 0.1557
Epoch 00015: loss improved from 0.46446 to 0.45838, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4584 - accuracy: 0.9684 - mse: 0.1557 - v
al_loss: 0.4812 - val_accuracy: 0.9518 - val_mse: 0.1609
Epoch 16/50
122/122 [==============================] - ETA: 0s - loss: 0.4531 - accuracy: 0.9693 - mse: 0.1532
Epoch 00016: loss improved from 0.45838 to 0.45313, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4531 - accuracy: 0.9693 - mse: 0.1532 - v
al_loss: 0.4837 - val_accuracy: 0.9485 - val_mse: 0.1602
Epoch 17/50
122/122 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.9720 - mse: 0.1501
Epoch 00017: loss improved from 0.45313 to 0.44612, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4461 - accuracy: 0.9720 - mse: 0.1501 - v
al_loss: 0.4722 - val_accuracy: 0.9516 - val_mse: 0.1568
Epoch 18/50
122/122 [==============================] - ETA: 0s - loss: 0.4378 - accuracy: 0.9753 - mse: 0.1468
Epoch 00018: loss improved from 0.44612 to 0.43777, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4378 - accuracy: 0.9753 - mse: 0.1468 - v
al_loss: 0.4803 - val_accuracy: 0.9409 - val_mse: 0.1590
Epoch 19/50
122/122 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.9732 - mse: 0.1457
Epoch 00019: loss improved from 0.43777 to 0.43679, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 550ms/step - loss: 0.4368 - accuracy: 0.9732 - mse: 0.1457 - v
al_loss: 0.4734 - val_accuracy: 0.9473 - val_mse: 0.1554
Epoch 20/50
122/122 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.9749 - mse: 0.1431
Epoch 00020: loss improved from 0.43679 to 0.43068, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4307 - accuracy: 0.9749 - mse: 0.1431 - v
al_loss: 0.4555 - val_accuracy: 0.9594 - val_mse: 0.1489
Epoch 21/50
122/122 [==============================] - ETA: 0s - loss: 0.4216 - accuracy: 0.9795 - mse: 0.1395
Epoch 00021: loss improved from 0.43068 to 0.42159, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4216 - accuracy: 0.9795 - mse: 0.1395 - v
al_loss: 0.4542 - val_accuracy: 0.9586 - val_mse: 0.1472
Epoch 22/50
122/122 [==============================] - ETA: 0s - loss: 0.4169 - accuracy: 0.9803 - mse: 0.1373
Epoch 00022: loss improved from 0.42159 to 0.41693, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4169 - accuracy: 0.9803 - mse: 0.1373 - v
al_loss: 0.4464 - val_accuracy: 0.9576 - val_mse: 0.1451
Epoch 23/50
122/122 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.9806 - mse: 0.1353
Epoch 00023: loss improved from 0.41693 to 0.41278, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4128 - accuracy: 0.9806 - mse: 0.1353 - v
al_loss: 0.4450 - val_accuracy: 0.9615 - val_mse: 0.1424
Epoch 24/50
122/122 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.9817 - mse: 0.1331
Epoch 00024: loss improved from 0.41278 to 0.40798, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.4080 - accuracy: 0.9817 - mse: 0.1331 - v
al_loss: 0.4405 - val_accuracy: 0.9604 - val_mse: 0.1410
Epoch 25/50
122/122 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.9793 - mse: 0.1324
Epoch 00025: loss did not improve from 0.40798
122/122 [==============================] - 66s 542ms/step - loss: 0.4080 - accuracy: 0.9793 - mse: 0.1324 - v
al_loss: 0.4312 - val_accuracy: 0.9629 - val_mse: 0.1382
Epoch 26/50
122/122 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.9822 - mse: 0.1293
Epoch 00026: loss improved from 0.40798 to 0.39975, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3997 - accuracy: 0.9822 - mse: 0.1293 - v
al_loss: 0.4349 - val_accuracy: 0.9633 - val_mse: 0.1369
Epoch 27/50
122/122 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.9830 - mse: 0.1273
Epoch 00027: loss improved from 0.39975 to 0.39521, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3952 - accuracy: 0.9830 - mse: 0.1273 - v
al_loss: 0.4307 - val_accuracy: 0.9617 - val_mse: 0.1354
Epoch 28/50
122/122 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.9845 - mse: 0.1250
Epoch 00028: loss improved from 0.39521 to 0.39068, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3907 - accuracy: 0.9845 - mse: 0.1250 - v
al_loss: 0.4257 - val_accuracy: 0.9622 - val_mse: 0.1336
Epoch 29/50
122/122 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.9851 - mse: 0.1230
Epoch 00029: loss improved from 0.39068 to 0.38609, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 548ms/step - loss: 0.3861 - accuracy: 0.9851 - mse: 0.1230 - v
al_loss: 0.4227 - val_accuracy: 0.9643 - val_mse: 0.1314
Epoch 30/50
122/122 [==============================] - 67s 549ms/step - loss: 0.3298 - accuracy: 0.9862 - mse: 0.0966 - v
al_loss: 0.3746 - val_accuracy: 0.9654 - val_mse: 0.1065
Epoch 47/50
122/122 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9861 - mse: 0.0953
Epoch 00047: loss improved from 0.32983 to 0.32700, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3270 - accuracy: 0.9861 - mse: 0.0953 - v
al_loss: 0.3658 - val_accuracy: 0.9648 - val_mse: 0.1051
Epoch 48/50
122/122 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.9862 - mse: 0.0938
Epoch 00048: loss improved from 0.32700 to 0.32385, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3239 - accuracy: 0.9862 - mse: 0.0938 - v
al_loss: 0.3656 - val_accuracy: 0.9654 - val_mse: 0.1037
Epoch 49/50
122/122 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.9862 - mse: 0.0925
Epoch 00049: loss improved from 0.32385 to 0.32086, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3209 - accuracy: 0.9862 - mse: 0.0925 - v
al_loss: 0.3653 - val_accuracy: 0.9657 - val_mse: 0.1025
Epoch 50/50
122/122 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.9862 - mse: 0.0911
Epoch 00050: loss improved from 0.32086 to 0.31793, saving model to ./weights\hs_v4_2_unet.hdf5
122/122 [==============================] - 67s 549ms/step - loss: 0.3179 - accuracy: 0.9862 - mse: 0.0911 - v
al_loss: 0.3606 - val_accuracy: 0.9653 - val_mse: 0.1012
predict test data
test candidate data : (20, 512, 512, 2)
 1/20 [>.............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is
 slow compared to the batch time (batch time: 0.0030s vs `on_predict_batch_end` time: 0.0389s). Check your ca
llbacks.
20/20 [==============================] - 1s 42ms/step
test result data : (20, 512, 512, 1)
array to image
PS C:\Users\HEESUNG\workspace\0.pp\unet-rgb-master>